{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/vlm-vfm-processing-pipeline/.venv/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/ubuntu/vlm-vfm-processing-pipeline/.venv/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vfm import VFM\n",
    "from scripts.torch_to_onnx import fix_onnx_fp16, convert_pytorch_to_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch -> ONNX conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cpu\"\n",
    "dtype = torch.float32\n",
    "# Instantiate the model\n",
    "model = VFM(device=device, dtype=dtype)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "all_pixel_values = torch.load(\n",
    "    \"/home/ubuntu/vlm-vfm-processing-pipeline/test_data/all_pixel_values.pkl\",\n",
    "    weights_only=True,\n",
    "    map_location=device,\n",
    ")\n",
    "patch_attn_mask = torch.load(\n",
    "    \"/home/ubuntu/vlm-vfm-processing-pipeline/test_data/patch_attn_mask.pkl\",\n",
    "    weights_only=True,\n",
    "    map_location=device,\n",
    ")\n",
    "tgt_sizes = torch.load(\n",
    "    \"/home/ubuntu/vlm-vfm-processing-pipeline/test_data/tgt_sizes.pkl\",\n",
    "    weights_only=True,\n",
    "    map_location=device,\n",
    ")\n",
    "\n",
    "# Set dynamic axes for inputs/outputs\n",
    "dynamic_axes = {\n",
    "    \"all_pixel_values\": {0: \"batch_size\"},\n",
    "    \"patch_attn_mask\": {0: \"batch_size\"},\n",
    "    \"tgt_sizes\": {0: \"batch_size\"},\n",
    "    \"vision_embedding\": {0: \"batch_size\"},\n",
    "}\n",
    "\n",
    "convert_pytorch_to_onnx(\n",
    "    model=model,\n",
    "    input_sample=(\n",
    "        all_pixel_values,\n",
    "        patch_attn_mask,\n",
    "        tgt_sizes\n",
    "        ),\n",
    "    onnx_path=\"models/vfm.onnx\",\n",
    "    input_names=[\"all_pixel_values\", \"patch_attn_mask\", \"tgt_sizes\"],\n",
    "    output_names=[\"vision_embedding\"],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=19,  # Higher opset for newer operators\n",
    "    export_params=True,  # Include model weights\n",
    "    do_constant_folding=True,  # Optimize constant operations\n",
    "    verbose=True,  # Show detailed conversion info\n",
    "    training=torch.onnx.TrainingMode.EVAL,  # Export in inference mode\n",
    "    enable_onnx_checker=True,  # Verify model structure\n",
    "    optimize_onnx=False,  # Apply optimizations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix_onnx_fp16\n",
    "\n",
    "Modify the onnx file to handle constants > FP16_Max and < FP16_Min . fix_onnx_fp16 is a helper function for this purpose.\n",
    "In the exported model, -inf is represented by the min value in FP32. The helper function modifies that to min in FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found constants out of FP16 range, clipped to FP16 range\n",
      "Saving modified onnx file at models/vfm_fix_outofrange_fp16.onnx\n"
     ]
    }
   ],
   "source": [
    "fp16_model_name = fix_onnx_fp16(\n",
    "    gen_models_path=\"models\",\n",
    "    model_base_name=\"vfm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QPC Compilation step\n",
    "- !This should run from inside the QC AI SDK Container\n",
    "qaic-exec cli tool is used to compile the model for Qualcomm AI Cloud 100. The input to this tool is onnx file generated above. The tool produces a QPC (Qualcomm Program Container) binary file in the path defined by -aic-binary-dir argument.\n",
    "\n",
    "Breakdown of key compile parameters.  \n",
    "We have compiled the onnx file\n",
    "\n",
    "* with 16 NSP cores\n",
    "* with float 16 precision\n",
    "* defined onnx symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: /opt/qti-aic/exec/qaic-exec: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!/opt/qti-aic/exec/qaic-exec \\\n",
    "    -m=/workspace/vfm_fix_outofrange_fp16.onnx \\\n",
    "    -aic-num-cores=16 -convert-to-fp16 \\\n",
    "    -onnx-define-symbol=batch_size,30 \\\n",
    "    -aic-binary-dir=/workspace/vfm_fix_outofrange_fp16_qpc \\\n",
    "    -aic-hw -aic-hw-version=2.0 -compile-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
